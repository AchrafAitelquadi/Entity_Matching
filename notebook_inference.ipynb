{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "256b5f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\envs\\ditto_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "from my_code.inference import  blocking_inference, run_blocked_inference\n",
    "from my_code.DK import GeneralDKInjector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a972b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(hp, model_path):\n",
    "    \"\"\"\n",
    "    Orchestrate the full blocking + inference pipeline.\n",
    "\n",
    "    1. Run the blocking stage to generate candidate pairs\n",
    "    2. Run the inference stage to score those pairs and\n",
    "       produce the final matched results\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hp : object\n",
    "        Hyperparameter/configuration object containing file paths,\n",
    "        thresholds, and other settings.\n",
    "    model_path : str\n",
    "        Path to the trained matching model to be used during inference.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Generate candidate pairs via blocking\n",
    "    blocking_inference(hp)\n",
    "\n",
    "    #Create DK object\n",
    "    if hp.dk:\n",
    "        dk = GeneralDKInjector(hp.task)\n",
    "    else:\n",
    "        dk = None\n",
    "\n",
    "    # Step 2: Score candidate pairs with the trained model and\n",
    "    #         write final matches to CSV\n",
    "    run_blocked_inference(\n",
    "                        model_path=model_path,\n",
    "                        blocked_pairs_csv=hp.output_pairs_csv,\n",
    "                        reference_table_csv=hp.table_reference_csv,\n",
    "                        source_table_csv=hp.table_source_csv,\n",
    "                        output_csv=hp.output_inference_csv,\n",
    "                        lm=hp.lm,\n",
    "                        max_len=hp.max_len,\n",
    "                        columns_to_use=hp.columns_to_use,\n",
    "                        inference_txt_path=hp.output_ditto_txt,\n",
    "                        dk = dk,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c78c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------------------------\n",
    "# Base path for storing all inference-related files\n",
    "# Update this path to your local project folder for inference outputs\n",
    "base_path_inference = \"D:/Study/ENSIAS/stage_2/ER/ditto/resultat_inference\"\n",
    "\n",
    "# -------------------- task Selection --------------------\n",
    "# Name of the current inference task.\n",
    "# You can choose any descriptive name (e.g., \"inference_1\", \"my_test_inference\")\n",
    "task_inference = \"inference_1\"\n",
    "\n",
    "# -------------------- Model checkpoint --------------------\n",
    "# Path to the trained Ditto model checkpoint to use for inference.\n",
    "# Make sure this path points to an existing .pt file from your training outputs\n",
    "model_path = \"D:/Study/ENSIAS/stage_2/ER/ditto/resultat_training/logs/Generated_data/model_Generated_data_bs32_ep1_lmdistilbert_alpha0.8_date2025-09-11.pt\"\n",
    "# ---------------------------------------------------------------------------------------------\n",
    "\n",
    "hp_inference = Namespace(\n",
    "    # -----------------------------------------------------------------------------------------\n",
    "    # Hyperparameters for the **blocking step**\n",
    "    model_name_blocking=\"all-MiniLM-L12-v2\",        # Sentence-transformers model used to create embeddings\n",
    "    top_k_blocking=5,                               # Keep the top-5 most similar candidates for each record\n",
    "    threshold_blocking=0.95,                        # Cosine similarity threshold to filter candidate pairs\n",
    "    batch_size_blocking=512,                        # Batch size for encoding/embedding computation\n",
    "    # -----------------------------------------------------------------------------------------\n",
    "\n",
    "    # Input CSV files (raw entity tables + ground truth for evaluation)\n",
    "    table_reference_csv=f\"{base_path_inference}/data/{task_inference}/reference_table.csv\",  # “Reference” table\n",
    "    table_source_csv=f\"{base_path_inference}/data/{task_inference}/source_table.csv\",        # “Source” table\n",
    "\n",
    "    # Ditto-style TXT files (entity tables converted to text for Ditto model consumption)\n",
    "    table_reference_txt=f\"{base_path_inference}/input_txt_blocking/{task_inference}/reference_table.txt\",\n",
    "    table_source_txt=f\"{base_path_inference}/input_txt_blocking/{task_inference}/source_table.txt\",\n",
    "\n",
    "    # Precomputed embedding vector files (to avoid recomputing sentence embeddings)\n",
    "    table_reference_vec=f\"{base_path_inference}/vectors_blocking/{task_inference}/reference_table.txt.mat\",\n",
    "    table_source_vec=f\"{base_path_inference}/vectors_blocking/{task_inference}/source_table.txt.mat\",\n",
    "\n",
    "    # Blocking outputs (candidate pairs produced by the blocking step)\n",
    "    output_pairs_csv=f\"{base_path_inference}/blocking/{task_inference}/blocking_pairs.csv\",          # Candidate pairs as CSV\n",
    "    output_ditto_txt=f\"{base_path_inference}/blocking/{task_inference}/blocking_pairs_ditto.txt\",    # Same pairs in Ditto TXT format\n",
    "\n",
    "    # Final inference result (predicted matches)\n",
    "    output_inference_csv=f\"{base_path_inference}/inference/{task_inference}/result.csv\",             # Final predicted matches\n",
    "\n",
    "    # Intermediate datasets for Ditto (if you need to re-run or inspect data)\n",
    "    dataset_csv_dir=f\"{base_path_inference}/dataset_ditto_csv\",\n",
    "    dataset_txt_dir=f\"{base_path_inference}/dataset_ditto_txt\",\n",
    "\n",
    "    # Misc task info\n",
    "    task=task_inference,           # Task name, used to organize all path references\n",
    "    lm=\"distilbert\",               # Language model backbone for Ditto inference\n",
    "    max_len=256,                   # Maximum token length for each input pair\n",
    "    dk = True,                     # Whether to inject domain knowledge (DK)\n",
    "    columns_to_use = None,         # Columns from source/reference tables to include (None = all)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b52251f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning mapping json file done.\n",
      "✅ Normalized and saved CSV at: D:/Study/ENSIAS/2eme_annee/stage_2/ER/ditto/resultat_inference/data/inference_1/reference_table.csv\n",
      "Cleaning mapping json file done.\n",
      "✅ Normalized and saved CSV at: D:/Study/ENSIAS/2eme_annee/stage_2/ER/ditto/resultat_inference/data/inference_1/source_table.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\envs\\ditto_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\DELL\\anaconda3\\envs\\ditto_env\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "100%|██████████| 1/1 [00:00<00:00, 500.75it/s]\n",
      "Processing blocked inference: 100%|██████████| 30/30 [00:10<00:00,  2.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved final results to: D:/Study/ENSIAS/2eme_annee/stage_2/ER/ditto/resultat_inference/inference/inference_1/result.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "main(hp_inference, model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ditto_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
