{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T08:09:04.401504Z",
     "iopub.status.busy": "2025-08-04T08:09:04.401281Z",
     "iopub.status.idle": "2025-08-04T08:09:04.419328Z",
     "shell.execute_reply": "2025-08-04T08:09:04.418634Z",
     "shell.execute_reply.started": "2025-08-04T08:09:04.401485Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import nltk\n",
    "import spacy\n",
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from faker import Faker\n",
    "from torch.utils import data\n",
    "from torch.optim import AdamW\n",
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import AutoModel, get_linear_schedule_with_warmup, AutoTokenizer\n",
    "nltk.download('stopwords')\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "stopwords = set(stopwords.words('english'))\n",
    "lm_mp = {\n",
    "    \"roberta\" : \"roberta-base\",\n",
    "    \"distilbert\" : \"distilbert-base-uncased\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T08:09:04.420902Z",
     "iopub.status.busy": "2025-08-04T08:09:04.420527Z",
     "iopub.status.idle": "2025-08-04T08:09:04.465514Z",
     "shell.execute_reply": "2025-08-04T08:09:04.464809Z",
     "shell.execute_reply.started": "2025-08-04T08:09:04.420886Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "faker = Faker('fr_FR')\n",
    "\n",
    "cities = ['Casablanca', 'Rabat', 'Marrakech', 'Agadir', 'Tanger', 'Oujda', 'Kenitra']\n",
    "\n",
    "def random_cin():\n",
    "    return faker.random_uppercase_letter() + faker.random_uppercase_letter() + str(faker.random_number(digits=6, fix_len=True))\n",
    "\n",
    "def random_cnss():\n",
    "    return str(faker.random_number(digits=8, fix_len=True))\n",
    "\n",
    "def introduce_typos(text):\n",
    "    \"\"\"Introduce realistic typos in text\"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return text\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # Common typo patterns\n",
    "    typo_patterns = [\n",
    "        # Character substitutions (keyboard proximity)\n",
    "        ('a', 'e'), ('e', 'a'), ('i', 'y'), ('o', '0'), ('0', 'o'),\n",
    "        ('u', 'i'), ('m', 'n'), ('n', 'm'), ('b', 'v'), ('v', 'b'),\n",
    "        ('c', 'k'), ('k', 'c'), ('s', 'z'), ('z', 's'),\n",
    "        # Double letters\n",
    "        ('l', 'll'), ('s', 'ss'), ('t', 'tt'), ('n', 'nn'),\n",
    "        # Missing letters (deletions)\n",
    "        ('th', 't'), ('ch', 'c'), ('qu', 'q'), ('tion', 'ion'),\n",
    "        # Transpositions\n",
    "        ('er', 're'), ('le', 'el'), ('on', 'no'), ('it', 'ti'),\n",
    "    ]\n",
    "    \n",
    "    # Apply random typos (30% chance)\n",
    "    if random.random() < 0.3:\n",
    "        # Choose random typo type\n",
    "        typo_type = random.choice(['substitute', 'delete', 'insert', 'transpose'])\n",
    "        \n",
    "        if typo_type == 'substitute' and len(text) > 1:\n",
    "            # Character substitution\n",
    "            pos = random.randint(0, len(text) - 1)\n",
    "            chars = list(text)\n",
    "            old_char = chars[pos].lower()\n",
    "            \n",
    "            # Use common substitutions or random\n",
    "            substitutions = {\n",
    "                'a': 'e', 'e': 'a', 'i': 'y', 'o': '0', 'u': 'i',\n",
    "                'm': 'n', 'n': 'm', 'b': 'v', 'v': 'b', 'c': 'k', 'k': 'c'\n",
    "            }\n",
    "            \n",
    "            if old_char in substitutions:\n",
    "                chars[pos] = substitutions[old_char]\n",
    "            else:\n",
    "                # Random nearby character\n",
    "                nearby_chars = 'abcdefghijklmnopqrstuvwxyz'\n",
    "                chars[pos] = random.choice(nearby_chars)\n",
    "            \n",
    "            text = ''.join(chars)\n",
    "        \n",
    "        elif typo_type == 'delete' and len(text) > 2:\n",
    "            # Delete a character\n",
    "            pos = random.randint(0, len(text) - 1)\n",
    "            text = text[:pos] + text[pos+1:]\n",
    "        \n",
    "        elif typo_type == 'insert':\n",
    "            # Insert a character\n",
    "            pos = random.randint(0, len(text))\n",
    "            char = random.choice('abcdefghijklmnopqrstuvwxyz')\n",
    "            text = text[:pos] + char + text[pos:]\n",
    "        \n",
    "        elif typo_type == 'transpose' and len(text) > 1:\n",
    "            # Transpose two adjacent characters\n",
    "            pos = random.randint(0, len(text) - 2)\n",
    "            chars = list(text)\n",
    "            chars[pos], chars[pos+1] = chars[pos+1], chars[pos]\n",
    "            text = ''.join(chars)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def add_formatting_issues(text):\n",
    "    \"\"\"Add realistic formatting issues\"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return text\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # Random formatting issues\n",
    "    if random.random() < 0.2:\n",
    "        # Extra spaces\n",
    "        text = re.sub(r'\\s+', '  ', text)\n",
    "    \n",
    "    if random.random() < 0.3:\n",
    "        # Missing spaces\n",
    "        text = text.replace(' ', '')\n",
    "    \n",
    "    if random.random() < 0.2:\n",
    "        # Random capitalization\n",
    "        if random.random() < 0.5:\n",
    "            text = text.upper()\n",
    "        else:\n",
    "            text = text.lower()\n",
    "    \n",
    "    if random.random() < 0.1:\n",
    "        # Add random punctuation\n",
    "        text = text + random.choice(['.', ',', ';', '!'])\n",
    "    \n",
    "    return text\n",
    "\n",
    "def corrupt_phone_number(phone):\n",
    "    \"\"\"Add realistic phone number corruption\"\"\"\n",
    "    if not phone or pd.isna(phone):\n",
    "        return phone\n",
    "    \n",
    "    phone = str(phone)\n",
    "    \n",
    "    # Common phone number issues\n",
    "    issues = [\n",
    "        lambda p: p.replace('+212', ''),  # Remove country code\n",
    "        lambda p: p.replace(' ', ''),     # Remove spaces\n",
    "        lambda p: p.replace('-', ''),     # Remove dashes\n",
    "        lambda p: p.replace('(', '').replace(')', ''),  # Remove parentheses\n",
    "        lambda p: '0' + p if not p.startswith('0') else p,  # Add leading zero\n",
    "        lambda p: p[1:] if p.startswith('0') else p,  # Remove leading zero\n",
    "        lambda p: p.replace('06', '6', 1),  # Remove leading 0 in mobile\n",
    "        lambda p: p.replace('05', '5', 1),  # Remove leading 0 in landline\n",
    "    ]\n",
    "    \n",
    "    # Apply 1-2 random issues\n",
    "    num_issues = random.randint(1, 2)\n",
    "    for _ in range(num_issues):\n",
    "        issue = random.choice(issues)\n",
    "        phone = issue(phone)\n",
    "    \n",
    "    # Add typos to phone numbers\n",
    "    phone = introduce_typos(phone)\n",
    "    \n",
    "    return phone\n",
    "\n",
    "def corrupt_email(email):\n",
    "    \"\"\"Add realistic email corruption\"\"\"\n",
    "    if not email or pd.isna(email):\n",
    "        return email\n",
    "    \n",
    "    email = str(email)\n",
    "    \n",
    "    # Common email issues\n",
    "    if random.random() < 0.3:\n",
    "        # Wrong domain\n",
    "        email = email.replace('@gmail.com', '@gmial.com')\n",
    "        email = email.replace('@yahoo.com', '@yaho.com')\n",
    "        email = email.replace('@hotmail.com', '@hotmial.com')\n",
    "    \n",
    "    if random.random() < 0.2:\n",
    "        # Missing @ or .\n",
    "        if '@' in email:\n",
    "            email = email.replace('@', '', 1)\n",
    "        elif '.' in email:\n",
    "            email = email.replace('.', '', 1)\n",
    "    \n",
    "    if random.random() < 0.2:\n",
    "        # Extra characters\n",
    "        email = email.replace('@', '@@', 1)\n",
    "    \n",
    "    # Add typos\n",
    "    email = introduce_typos(email)\n",
    "    \n",
    "    return email\n",
    "\n",
    "def corrupt_address(address):\n",
    "    \"\"\"Add realistic address corruption\"\"\"\n",
    "    if not address or pd.isna(address):\n",
    "        return address\n",
    "    \n",
    "    address = str(address)\n",
    "    \n",
    "    # Common address abbreviations and issues\n",
    "    abbreviations = {\n",
    "        'Avenue': 'Ave', 'Boulevard': 'Blvd', 'Street': 'St', 'Road': 'Rd',\n",
    "        'Rue': 'R.', 'Avenue': 'Av.', 'Boulevard': 'Bd.', 'Place': 'Pl.',\n",
    "        'Quartier': 'Q.', 'Résidence': 'Rés.', 'Immeuble': 'Imm.',\n",
    "        'Appartement': 'Apt', 'Numéro': 'N°', 'Bis': 'B'\n",
    "    }\n",
    "    \n",
    "    # Apply abbreviations\n",
    "    if random.random() < 0.4:\n",
    "        for full, abbr in abbreviations.items():\n",
    "            if full in address:\n",
    "                address = address.replace(full, abbr)\n",
    "    \n",
    "    # Add typos\n",
    "    address = introduce_typos(address)\n",
    "    \n",
    "    # Add formatting issues\n",
    "    address = add_formatting_issues(address)\n",
    "    \n",
    "    return address\n",
    "\n",
    "def add_data_entry_errors(person):\n",
    "    \"\"\"Add realistic data entry errors\"\"\"\n",
    "    modified = person.copy()\n",
    "    \n",
    "    # Name issues\n",
    "    if random.random() < 0.4:\n",
    "        modified[\"full_name\"] = introduce_typos(person[\"full_name\"])\n",
    "    \n",
    "    if random.random() < 0.3:\n",
    "        modified[\"full_name\"] = add_formatting_issues(person[\"full_name\"])\n",
    "    \n",
    "    # Email corruption\n",
    "    if random.random() < 0.3:\n",
    "        modified[\"email\"] = corrupt_email(person[\"email\"])\n",
    "    \n",
    "    # Phone corruption\n",
    "    if random.random() < 0.4:\n",
    "        modified[\"phone\"] = corrupt_phone_number(person[\"phone\"])\n",
    "    \n",
    "    # Address corruption\n",
    "    if random.random() < 0.4:\n",
    "        modified[\"address\"] = corrupt_address(person[\"address\"])\n",
    "    \n",
    "    # Employer name issues\n",
    "    if random.random() < 0.3:\n",
    "        modified[\"employer_name\"] = introduce_typos(person[\"employer_name\"])\n",
    "    \n",
    "    # CIN format variations\n",
    "    if random.random() < 0.2:\n",
    "        cin = person[\"cin\"]\n",
    "        if cin:\n",
    "            # Remove letters or add spaces\n",
    "            if random.random() < 0.5:\n",
    "                modified[\"cin\"] = cin.replace(cin[:2], cin[:2].lower())\n",
    "            else:\n",
    "                modified[\"cin\"] = cin[:2] + \" \" + cin[2:]\n",
    "    \n",
    "    # CNSS number issues\n",
    "    if random.random() < 0.2:\n",
    "        cnss = person[\"cnss_number\"]\n",
    "        if cnss:\n",
    "            # Add dashes or spaces\n",
    "            if len(cnss) == 8:\n",
    "                modified[\"cnss_number\"] = cnss[:4] + \"-\" + cnss[4:]\n",
    "    \n",
    "    # Date format variations\n",
    "    if random.random() < 0.2:\n",
    "        dob = person[\"date_of_birth\"]\n",
    "        if dob:\n",
    "            # Change date format\n",
    "            from datetime import datetime\n",
    "            try:\n",
    "                date_obj = datetime.fromisoformat(dob)\n",
    "                formats = [\n",
    "                    \"%d/%m/%Y\", \"%d-%m-%Y\", \"%Y/%m/%d\", \"%m/%d/%Y\"\n",
    "                ]\n",
    "                new_format = random.choice(formats)\n",
    "                modified[\"date_of_birth\"] = date_obj.strftime(new_format)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    return modified\n",
    "\n",
    "def generate_person():\n",
    "    return {\n",
    "        \"full_name\": faker.name(),\n",
    "        \"cin\": random_cin(),\n",
    "        \"date_of_birth\": faker.date_of_birth(minimum_age=18, maximum_age=65).isoformat(),\n",
    "        \"place_of_birth\": random.choice(cities),\n",
    "        \"cnss_number\": random_cnss(),\n",
    "        \"email\": faker.email(),\n",
    "        \"phone\": faker.phone_number(),\n",
    "        \"address\": faker.street_address(),\n",
    "        \"city\": random.choice(cities),\n",
    "        \"employer_name\": faker.company()\n",
    "    }\n",
    "\n",
    "def add_light_noise(person):\n",
    "    \"\"\"Add light noise to reference table\"\"\"\n",
    "    modified = person.copy()\n",
    "    \n",
    "    # Light modifications (20% chance each)\n",
    "    if random.random() < 0.2:\n",
    "        modified[\"full_name\"] = person[\"full_name\"].title()  # case change\n",
    "    \n",
    "    if random.random() < 0.1:\n",
    "        modified[\"email\"] = person[\"email\"].lower()\n",
    "    \n",
    "    if random.random() < 0.1:\n",
    "        modified[\"phone\"] = person[\"phone\"].replace(\" \", \"\")  # remove spaces\n",
    "    \n",
    "    return modified\n",
    "\n",
    "def add_heavy_noise(person):\n",
    "    \"\"\"Add heavy noise to dataset table with realistic issues\"\"\"\n",
    "    modified = person.copy()\n",
    "    \n",
    "    # Apply data entry errors first\n",
    "    modified = add_data_entry_errors(modified)\n",
    "    \n",
    "    # Additional heavy modifications\n",
    "    if random.random() < 0.6:\n",
    "        modified[\"full_name\"] = modified[\"full_name\"].lower()  # case change\n",
    "    \n",
    "    if random.random() < 0.4:\n",
    "        modified[\"address\"] = modified[\"address\"].replace(\"Rue\", \"R.\")  # abbreviation\n",
    "    \n",
    "    if random.random() < 0.4:\n",
    "        employer = modified[\"employer_name\"]\n",
    "        if employer and len(employer.split()) > 1:\n",
    "            modified[\"employer_name\"] = employer.split()[0]  # first word only\n",
    "    \n",
    "    # Add missing values (realistic missing data patterns)\n",
    "    missing_probabilities = {\n",
    "        'email': 0.15,      # Email often missing\n",
    "        'phone': 0.1,       # Phone sometimes missing\n",
    "        'address': 0.2,     # Address frequently incomplete\n",
    "        'employer_name': 0.25,  # Employer often missing\n",
    "        'cnss_number': 0.3,  # CNSS often missing in informal sectors\n",
    "    }\n",
    "    \n",
    "    for field, prob in missing_probabilities.items():\n",
    "        if random.random() < prob:\n",
    "            modified[field] = None\n",
    "    \n",
    "    # Partial data corruption (common in real datasets)\n",
    "    if random.random() < 0.1:\n",
    "        # Truncated fields\n",
    "        if modified[\"full_name\"] and len(modified[\"full_name\"]) > 10:\n",
    "            modified[\"full_name\"] = modified[\"full_name\"][:10] + \"...\"\n",
    "    \n",
    "    if random.random() < 0.05:\n",
    "        # Wrong data type (numbers as strings, etc.)\n",
    "        if modified[\"phone\"]:\n",
    "            modified[\"phone\"] = \"TEL: \" + str(modified[\"phone\"])\n",
    "    \n",
    "    return modified\n",
    "\n",
    "def create_similar_person(base_person):\n",
    "    \"\"\"Create a person that looks similar but is different (hard negative)\"\"\"\n",
    "    similar = generate_person()\n",
    "    \n",
    "    # Keep some similar characteristics to make it tricky\n",
    "    if random.random() < 0.3:\n",
    "        # Same city\n",
    "        similar[\"city\"] = base_person[\"city\"]\n",
    "        similar[\"place_of_birth\"] = base_person[\"place_of_birth\"]\n",
    "    \n",
    "    if random.random() < 0.2:\n",
    "        # Similar name (same first name or last name)\n",
    "        base_name_parts = base_person[\"full_name\"].split()\n",
    "        similar_name_parts = similar[\"full_name\"].split()\n",
    "        if len(base_name_parts) >= 2 and len(similar_name_parts) >= 2:\n",
    "            # Same first name, different last name\n",
    "            similar[\"full_name\"] = base_name_parts[0] + \" \" + similar_name_parts[-1]\n",
    "    \n",
    "    if random.random() < 0.1:\n",
    "        # Similar employer\n",
    "        similar[\"employer_name\"] = base_person[\"employer_name\"]\n",
    "    \n",
    "    if random.random() < 0.05:\n",
    "        # Similar phone prefix (same area code)\n",
    "        if base_person[\"phone\"] and similar[\"phone\"]:\n",
    "            base_prefix = base_person[\"phone\"][:3]\n",
    "            similar[\"phone\"] = base_prefix + similar[\"phone\"][3:]\n",
    "    \n",
    "    return similar\n",
    "\n",
    "def generate_duplicate_variations(person):\n",
    "    \"\"\"Generate multiple variations of the same person (realistic duplicates)\"\"\"\n",
    "    variations = []\n",
    "    \n",
    "    # Create 2-4 variations\n",
    "    num_variations = random.randint(2, 4)\n",
    "    \n",
    "    for _ in range(num_variations):\n",
    "        variation = person.copy()\n",
    "        \n",
    "        # Apply different levels of corruption\n",
    "        corruption_level = random.choice(['light', 'medium', 'heavy'])\n",
    "        \n",
    "        if corruption_level == 'light':\n",
    "            variation = add_light_noise(variation)\n",
    "        elif corruption_level == 'medium':\n",
    "            variation = add_data_entry_errors(variation)\n",
    "        else:  # heavy\n",
    "            variation = add_heavy_noise(variation)\n",
    "        \n",
    "        variations.append(variation)\n",
    "    \n",
    "    return variations\n",
    "\n",
    "def generate_tables(base_path, n_total=2000, match_ratio=0.3):\n",
    "    \"\"\"Generate reference table (cleaner) and source table (noisy)\"\"\"\n",
    "    n_matches = int(n_total * match_ratio)\n",
    "    n_non_matches = n_total - n_matches\n",
    "\n",
    "    print(f\"Generating {n_total} records:\")\n",
    "    print(f\"{n_matches} matching pairs ({match_ratio*100:.0f}%)\")\n",
    "    print(f\"{n_non_matches} non-matching records\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Create folders\n",
    "    os.makedirs(os.path.join(base_path, \"data\"), exist_ok=True)\n",
    "\n",
    "    # Generate base persons\n",
    "    base_persons = [generate_person() for _ in range(n_matches)]\n",
    "\n",
    "    # Reference table: light noise\n",
    "    reference_table = []\n",
    "    for i, person in enumerate(base_persons):\n",
    "        ref_person = add_light_noise(person)\n",
    "        ref_person['id'] = f\"REF_{i:04d}\"\n",
    "        reference_table.append(ref_person)\n",
    "\n",
    "    # Source table: heavy noise + hard negatives\n",
    "    source_table = []\n",
    "    for i, person in enumerate(base_persons):\n",
    "        src_person = add_heavy_noise(person)\n",
    "        src_person['id'] = f\"SRC_{i:04d}\"\n",
    "        source_table.append(src_person)\n",
    "\n",
    "    for i in range(n_matches, n_total):\n",
    "        if random.random() < 0.3:\n",
    "            base_for_similar = random.choice(base_persons)\n",
    "            person = create_similar_person(base_for_similar)\n",
    "        else:\n",
    "            person = generate_person()\n",
    "\n",
    "        if random.random() < 0.5:\n",
    "            person = add_heavy_noise(person)\n",
    "\n",
    "        person['id'] = f\"SRC_{i:04d}\"\n",
    "        source_table.append(person)\n",
    "\n",
    "    # Ground truth\n",
    "    ground_truth = []\n",
    "    for i in range(n_matches):\n",
    "        ground_truth.append({\n",
    "            'ref_id': f\"REF_{i:04d}\",\n",
    "            'data_id': f\"SRC_{i:04d}\",\n",
    "            'match': 1\n",
    "        })\n",
    "\n",
    "    for _ in range(n_matches * 2):\n",
    "        ref_idx = random.randint(0, len(reference_table) - 1)\n",
    "        data_idx = random.randint(0, len(source_table) - 1)\n",
    "        ref_id = reference_table[ref_idx]['id']\n",
    "        data_id = source_table[data_idx]['id']\n",
    "\n",
    "        if not any(gt['ref_id'] == ref_id and gt['data_id'] == data_id for gt in ground_truth):\n",
    "            ground_truth.append({'ref_id': ref_id, 'data_id': data_id, 'match': 0})\n",
    "\n",
    "    # Shuffle\n",
    "    random.shuffle(reference_table)\n",
    "    random.shuffle(source_table)\n",
    "\n",
    "    # Save\n",
    "    pd.DataFrame(reference_table).to_csv(os.path.join(base_path, \"data\", \"reference_table.csv\"), index=False)\n",
    "    pd.DataFrame(source_table).to_csv(os.path.join(base_path, \"data\", \"source_table.csv\"), index=False)\n",
    "    pd.DataFrame(ground_truth).to_csv(os.path.join(base_path, \"data\", \"ground_truth.csv\"), index=False)\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\n📁 Files saved\")\n",
    "    print(f\" - Reference: {len(reference_table)} rows\")\n",
    "    print(f\" - Source:    {len(source_table)} rows\")\n",
    "    print(f\" - Ground Truth: {len(ground_truth)} pairs\")\n",
    "    print(f\"   - Matches: {n_matches}\")\n",
    "    print(f\"   - Non-matches: {len(ground_truth) - n_matches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T08:09:04.466523Z",
     "iopub.status.busy": "2025-08-04T08:09:04.466277Z",
     "iopub.status.idle": "2025-08-04T08:09:04.483098Z",
     "shell.execute_reply": "2025-08-04T08:09:04.482483Z",
     "shell.execute_reply.started": "2025-08-04T08:09:04.466503Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ----------- Step 1: Convert CSV to Ditto-style text -----------\n",
    "\n",
    "def normalize_word(word):\n",
    "    word = word.lower()\n",
    "    word = unicodedata.normalize('NFD', word)\n",
    "    word = ''.join([char for char in word if unicodedata.category(char) != 'Mn'])\n",
    "    return word\n",
    "    \n",
    "def csv_to_ditto_txt(csv_path, out_txt_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    os.makedirs(os.path.dirname(out_txt_path), exist_ok=True)\n",
    "    \n",
    "    with open(out_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for _, row in df.iterrows():\n",
    "            fields = [f\"COL {normalize_word(col)} VAL {normalize_word(str(val))}\" for col, val in row.items() if col != \"id\"]\n",
    "            line = \" \".join(fields)\n",
    "            f.write(line + \"\\n\")\n",
    "\n",
    "# ----------- Step 2: Encode lines using Transformer -----------\n",
    "\n",
    "def encode_all(input_path, out_path, model, overwrite=True):\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "    with open(input_path, encoding='utf-8') as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    if not os.path.exists(out_path) or overwrite:\n",
    "        vectors = model.encode(lines, normalize_embeddings=True)\n",
    "        with open(out_path, \"wb\") as f:\n",
    "            pickle.dump(vectors, f)\n",
    "    else:\n",
    "        with open(out_path, \"rb\") as f:\n",
    "            vectors = pickle.load(f)\n",
    "\n",
    "    return lines, vectors\n",
    "\n",
    "# ----------- Step 3: Perform blocked matrix multiplication -----------\n",
    "\n",
    "def blocked_matmul(mata, matb, threshold=0.95, k=3, batch_size=1024):\n",
    "    mata = np.array(mata)\n",
    "    matb = np.array(matb)\n",
    "    results = []\n",
    "\n",
    "    for start in tqdm(range(0, len(matb), batch_size)):\n",
    "        block = matb[start:start + batch_size]\n",
    "        sim_mat = np.matmul(mata, block.T)\n",
    "        for j in range(sim_mat.shape[1]):\n",
    "            top_k_idx = np.argpartition(-sim_mat[:, j], k)[:k]\n",
    "            for i in top_k_idx:\n",
    "                sim_score = sim_mat[i][j]\n",
    "                if sim_score > threshold:\n",
    "                    results.append((i, j + start, sim_score, 1))\n",
    "                else:\n",
    "                    results.append((i, j + start, sim_score, 0))\n",
    "        \n",
    "    return results\n",
    "\n",
    "# ----------- Step 4: Save outputs -----------\n",
    "\n",
    "def dump_pairs_csv(out_fn, pairs):\n",
    "    df = pd.DataFrame(pairs, columns=[\"id_table_a\", \"id_table_b\", \"similarity\", \"label\"])\n",
    "    df.to_csv(out_fn, index=False)\n",
    "\n",
    "def dump_ditto_txt(out_fn, pairs, entries_a, entries_b):\n",
    "    with open(out_fn, \"w\", encoding=\"utf-8\") as f:\n",
    "        for idx_a, idx_b, _, label in pairs:\n",
    "            idx_a = int(idx_a)  # convert from float to int\n",
    "            idx_b = int(idx_b)\n",
    "            row = f\"{entries_a[idx_a]}\\t{entries_b[idx_b]}\\t{int(label)}\\n\"\n",
    "            f.write(row)\n",
    "\n",
    "def evaluate_blocking_metrics(pairs, ground_truth_path, ref_table_path, data_table_path):\n",
    "    gt_df = pd.read_csv(ground_truth_path)\n",
    "    ref_df = pd.read_csv(ref_table_path)\n",
    "    data_df = pd.read_csv(data_table_path)\n",
    "\n",
    "    # Build index-to-id maps (based on order of encoding)\n",
    "    ref_id_map = {i: row[\"id\"] for i, row in ref_df.reset_index().iterrows()}\n",
    "    data_id_map = {i: row[\"id\"] for i, row in data_df.reset_index().iterrows()}\n",
    "\n",
    "    # Set of true matches from ground truth\n",
    "    true_matches = set(zip(gt_df[gt_df[\"match\"] == 1][\"ref_id\"], gt_df[gt_df[\"match\"] == 1][\"data_id\"]))\n",
    "\n",
    "    # Set of predicted matches from blocking\n",
    "    predicted_pairs = set((ref_id_map[i], data_id_map[j]) for i, j, _, _ in pairs)\n",
    "\n",
    "    # Set of predicted positives (sim > threshold AND labeled 1 in blocking)\n",
    "    predicted_positive = set((ref_id_map[i], data_id_map[j]) for i, j, _, label in pairs if label == 1)\n",
    "\n",
    "    # Recall = % of true matches retrieved\n",
    "    found_matches = true_matches.intersection(predicted_positive)\n",
    "    recall = len(found_matches) / len(true_matches) if true_matches else 0\n",
    "\n",
    "    # Precision = % of predicted matches that are true matches\n",
    "    precision = len(found_matches) / len(predicted_positive) if predicted_positive else 0\n",
    "\n",
    "    # Reduction Ratio = 1 - (# blocked pairs / all possible pairs)\n",
    "    total_possible_pairs = len(ref_df) * len(data_df)\n",
    "    rr = 1 - (len(pairs) / total_possible_pairs)\n",
    "\n",
    "    print(\"\\n📊 Blocking Metrics:\")\n",
    "    print(f\" - Total candidate pairs generated: {len(pairs)}\")\n",
    "    print(f\" - Total true matches in ground truth: {len(true_matches)}\")\n",
    "    print(f\" - Predicted positive pairs (label=1): {len(predicted_positive)}\")\n",
    "    print(f\" - Correctly predicted matches: {len(found_matches)}\")\n",
    "    print(f\" - Recall:           {recall:.4f}\")\n",
    "    print(f\" - Precision:        {precision:.4f}\")\n",
    "    print(f\" - Reduction Ratio:  {rr:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"pairs_generated\": len(pairs),\n",
    "        \"true_matches\": len(true_matches),\n",
    "        \"predicted_positive\": len(predicted_positive),\n",
    "        \"found_matches\": len(found_matches),\n",
    "        \"recall\": recall,\n",
    "        \"precision\": precision,\n",
    "        \"reduction_ratio\": rr\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T08:09:04.484184Z",
     "iopub.status.busy": "2025-08-04T08:09:04.483912Z",
     "iopub.status.idle": "2025-08-04T08:09:04.501778Z",
     "shell.execute_reply": "2025-08-04T08:09:04.501170Z",
     "shell.execute_reply.started": "2025-08-04T08:09:04.484162Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_blocking(hp):\n",
    "    # Step 1: Generate Ditto-style .txt\n",
    "    csv_to_ditto_txt(hp.table_reference_csv, hp.table_reference_txt)\n",
    "    csv_to_ditto_txt(hp.table_source_csv, hp.table_source_txt)\n",
    "\n",
    "    # Step 2: Load model\n",
    "    model = SentenceTransformer(hp.model_name_blocking)\n",
    "\n",
    "    # Step 3: Encode and save vectors\n",
    "    entries_ref, vecs_ref = encode_all(hp.table_reference_txt, hp.table_reference_vec, model)\n",
    "    entries_src, vecs_src = encode_all(hp.table_source_txt, hp.table_source_vec, model)\n",
    "\n",
    "    # Step 4: Run blocking\n",
    "    pairs = blocked_matmul(\n",
    "        vecs_ref, vecs_src,\n",
    "        threshold=hp.threshold_blocking,\n",
    "        k=hp.top_k_blocking,\n",
    "        batch_size=hp.batch_size_blocking\n",
    "    )\n",
    "\n",
    "    # Step 5: Evaluate\n",
    "    evaluate_blocking_metrics(\n",
    "        pairs,\n",
    "        ground_truth_path=hp.ground_truth_csv,\n",
    "        ref_table_path=hp.table_reference_csv,\n",
    "        data_table_path=hp.table_source_csv\n",
    "    )\n",
    "\n",
    "    positive_pairs = [p for p in pairs if p[3] == 1]\n",
    "    negative_pairs = [p for p in pairs if p[3] == 0]\n",
    "\n",
    "    min_len = min(len(positive_pairs), len(negative_pairs))\n",
    "    random.seed(42)\n",
    "    positive_pairs = random.sample(positive_pairs, min_len)\n",
    "    negative_pairs = random.sample(negative_pairs, min_len)\n",
    "    pairs = positive_pairs + negative_pairs\n",
    "    random.shuffle(pairs)\n",
    "    \n",
    "    # Step 6: Save full pairs output\n",
    "    os.makedirs(os.path.dirname(hp.output_pairs_csv), exist_ok=True)\n",
    "    dump_pairs_csv(hp.output_pairs_csv, pairs)\n",
    "    dump_ditto_txt(hp.output_ditto_txt, pairs, entries_ref, entries_src)\n",
    "\n",
    "    print(f\"\\n✅ Blocking completed: {len(pairs)} balanced pairs written to:\")\n",
    "\n",
    "    dataset_csv_dir = hp.dataset_csv_dir\n",
    "    dataset_txt_dir = hp.dataset_txt_dir\n",
    "    os.makedirs(dataset_csv_dir, exist_ok=True)\n",
    "    os.makedirs(dataset_txt_dir, exist_ok=True)\n",
    "\n",
    "    df = pd.read_csv(hp.output_pairs_csv)\n",
    "    train, temp = train_test_split(df, test_size=0.4, stratify=df['label'], random_state=42)\n",
    "    valid, test = train_test_split(temp, test_size=0.5, stratify=temp['label'], random_state=42)\n",
    "\n",
    "    datasets = {'train': train, 'valid': valid, 'test': test}\n",
    "\n",
    "    for split, split_df in datasets.items():\n",
    "        split_csv = f\"{dataset_csv_dir}/{split}.csv\"\n",
    "        split_txt = f\"{dataset_txt_dir}/{split}.txt\"\n",
    "        split_df.to_csv(split_csv, index=False)\n",
    "        dump_ditto_txt(split_txt, split_df.values.tolist(), entries_ref, entries_src)\n",
    "\n",
    "    print(\"\\n📁 Split saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T08:09:04.502833Z",
     "iopub.status.busy": "2025-08-04T08:09:04.502580Z",
     "iopub.status.idle": "2025-08-04T08:09:04.526358Z",
     "shell.execute_reply": "2025-08-04T08:09:04.525849Z",
     "shell.execute_reply.started": "2025-08-04T08:09:04.502812Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Augmenter:\n",
    "    \"\"\"Data augmentation operator.\n",
    "\n",
    "    Support both span and attribute level augmentation operators.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def sample_span(self, tokens, labels, span_len=3):\n",
    "        candidates = []\n",
    "        for idx, token in enumerate(tokens):\n",
    "            if idx + span_len - 1 < len(labels) and ''.join(labels[idx:idx + span_len]) == 'O'*span_len:\n",
    "                candidates.append((idx, idx + span_len - 1))\n",
    "\n",
    "        if len(candidates) <= 0:\n",
    "            return -1, -1\n",
    "\n",
    "        return random.choice(candidates)\n",
    "\n",
    "    def sample_position(self, tokens, labels, tfidf=False):\n",
    "        candidates = []\n",
    "        for idx, token in enumerate(tokens):\n",
    "            if labels[idx] == 'O':\n",
    "                candidates.append(idx)\n",
    "        if len(candidates) <= 0:\n",
    "            return -1\n",
    "\n",
    "        return random.choice(candidates)\n",
    "\n",
    "    def augment(self, tokens, labels, op=\"del\"):\n",
    "        \"\"\" Performs data augmentation on a sequence of tokens\n",
    "\n",
    "        The supported ops:\n",
    "           ['del', 'drop_col',\n",
    "            'append_col', 'drop_token',\n",
    "            'drop_len',\n",
    "            'drop_sym',\n",
    "            'drop_same',\n",
    "            'swap',\n",
    "            'ins',\n",
    "            'all']\n",
    "\n",
    "        Args:\n",
    "            tokens (list of strings): the input tokens\n",
    "            labels (list of strings): the labels of the tokens\n",
    "            op (str, optional): a string encoding of the operator to be applied\n",
    "\n",
    "        Returns:\n",
    "            list of strings: the augmented tokens\n",
    "            list of strings: the augmented labels\n",
    "        \"\"\"\n",
    "        if 'del' in op:\n",
    "\n",
    "            span_len = random.randint(1, 2)\n",
    "            pos1, pos2 = self.sample_span(tokens, labels, span_len)\n",
    "\n",
    "            if pos1 < 0:\n",
    "                return tokens, labels\n",
    "\n",
    "            new_tokens = tokens[:pos1] + tokens[pos2+1:]\n",
    "            new_labels = labels[:pos1] + labels[pos2+1:]\n",
    "        elif 'swap' in op:\n",
    "            span_len = random.randint(2, 4)\n",
    "            pos1, pos2 = self.sample_span(tokens, labels, span_len)\n",
    "            if pos1 < 0:\n",
    "                return tokens, labels\n",
    "\n",
    "            sub_arr = tokens[pos1: pos2 + 1]\n",
    "            random.shuffle(sub_arr)\n",
    "            new_tokens = tokens[:pos1] + sub_arr + tokens[pos2 + 1:]\n",
    "            new_labels = labels\n",
    "\n",
    "        elif 'drop_len' in op:\n",
    "            # drop tokens below a certain length\n",
    "            all_lens = [len(token) for token, label in zip(tokens, labels) if label == 'O']\n",
    "\n",
    "            if len(all_lens) == 0:\n",
    "                return tokens, labels\n",
    "\n",
    "            target_lens = random.choices(all_lens, k=1)\n",
    "            new_tokens = []\n",
    "            new_labels = []\n",
    "\n",
    "            for token, label in zip(tokens, labels):\n",
    "                if label != 'O' or len(token) not in target_lens:\n",
    "                    new_tokens.append(token)\n",
    "                    new_labels.append(label)\n",
    "\n",
    "            return new_tokens, new_labels\n",
    "\n",
    "        elif 'drop_sym' in op:\n",
    "            def drop_sym(token):\n",
    "                return ''.join([ch if ch.isalnum() else ' ' for ch in token])\n",
    "\n",
    "            dropped_tokens = [drop_sym(token) for token in tokens]\n",
    "            new_tokens = []\n",
    "            new_labels = []\n",
    "            for token, d_token, label in zip(tokens, dropped_tokens, labels):\n",
    "                if random.randint(0, 4) != 0 or label != 'O':\n",
    "                    new_tokens.append(token)\n",
    "                    new_labels.append(label)\n",
    "                else:\n",
    "                    if d_token != '':\n",
    "                        new_tokens.append(d_token)\n",
    "                        new_labels.append(label)\n",
    "            return new_tokens, new_labels\n",
    "\n",
    "        elif 'drop_same' in op:\n",
    "            left_token = set([])\n",
    "            right_token = set([])\n",
    "            left = True\n",
    "            for token, label in zip(tokens, labels):\n",
    "                if label == 'O':\n",
    "                    token = token.lower()\n",
    "                    if left:\n",
    "                        left_token.add(token)\n",
    "                    else:\n",
    "                        right_token.add(token)\n",
    "\n",
    "                if label == \"[SEP]\":\n",
    "                    left = False\n",
    "\n",
    "            same = left_token & right_token #intersection\n",
    "            targets = random.choices(list(same), k = 1) if same else []\n",
    "            new_tokens, new_labels = [], []\n",
    "            for token, label in zip(tokens, labels):\n",
    "                if token.lower() not in targets or label != 'O':\n",
    "                    new_tokens.append(token)\n",
    "                    new_labels.append(label)\n",
    "            return new_tokens, new_labels\n",
    "\n",
    "        elif 'drop_token' in op:\n",
    "            new_tokens, new_labels = [], []\n",
    "            for token, label in zip(tokens, labels):\n",
    "                if label != 'O' or random.randint(0, 4) != 0:\n",
    "                    new_tokens.append(token)\n",
    "                    new_labels.append(label)\n",
    "            return new_tokens, new_labels\n",
    "\n",
    "        elif \"ins\" in op:\n",
    "            pos = self.sample_position(tokens, labels)\n",
    "            symbol = random.choice('-*.,#&')\n",
    "            new_tokens = tokens[:pos] + [symbol] + tokens[pos:]\n",
    "            new_labels = labels[:pos] + ['O'] + labels[pos:]\n",
    "            return new_tokens, new_labels\n",
    "        elif \"append_col\" in op:\n",
    "            col_starts = [i for i in range(len(tokens)) if tokens[i] == 'COL']\n",
    "            col_ends = [0] * len(col_starts)\n",
    "            col_lens = [0] * len(col_starts)\n",
    "            for i, pos in enumerate(col_starts):\n",
    "                if i == len(col_starts) - 1:\n",
    "                    col_lens[i] = len(tokens) - pos\n",
    "                    col_ends[i] = len(tokens) - 1\n",
    "                else:\n",
    "                    col_lens[i] = col_starts[i + 1] - pos\n",
    "                    col_ends[i] = col_starts[i + 1] - 1\n",
    "\n",
    "                if tokens[col_ends[i]] == '[SEP]':\n",
    "                    col_ends[i] -= 1\n",
    "                    col_lens[i] -= 1\n",
    "                    break\n",
    "            candidates = [i for i, le in enumerate(col_lens) if le > 0]\n",
    "            if len(candidates) >= 2:\n",
    "                idx1, idx2 = random.sample(candidates,k=2)\n",
    "                start1, end1 = col_starts[idx1], col_ends[idx1]\n",
    "                sub_tokens = tokens[start1:end1+1]\n",
    "                sub_labels = labels[start1:end1+1]\n",
    "                val_pos = 0\n",
    "                for i, token in enumerate(sub_tokens):\n",
    "                    if token == 'VAL':\n",
    "                        val_pos = i + 1\n",
    "                        break\n",
    "                sub_tokens = sub_tokens[val_pos:]\n",
    "                sub_labels = sub_labels[val_pos:]\n",
    "\n",
    "                end2 = col_ends[idx2]\n",
    "                new_tokens = []\n",
    "                new_labels = []\n",
    "                for i in range(len(tokens)):\n",
    "                    if start1 <= i <= end1:\n",
    "                        continue\n",
    "                    new_tokens.append(tokens[i])\n",
    "                    new_labels.append(labels[i])\n",
    "                    if i == end2:\n",
    "                        new_tokens += sub_tokens\n",
    "                        new_labels += sub_labels\n",
    "                return new_tokens, new_labels\n",
    "            else:\n",
    "                new_tokens, new_labels = tokens, labels\n",
    "        elif 'drop_col' in op:\n",
    "            col_starts = [i for i in range(len(tokens)) if tokens[i] == 'COL']\n",
    "            col_ends = [0] * len(col_starts)\n",
    "            col_lens = [0] * len(col_starts)\n",
    "            for i, pos in enumerate(col_starts):\n",
    "                if i == len(col_starts) - 1:\n",
    "                    col_lens[i] = len(tokens) - pos\n",
    "                    col_ends[i] = len(tokens) - 1\n",
    "                else:\n",
    "                    col_lens[i] = col_starts[i + 1] - pos\n",
    "                    col_ends[i] = col_starts[i + 1] - 1\n",
    "\n",
    "                if tokens[col_ends[i]] == '[SEP]':\n",
    "                    col_ends[i] -= 1\n",
    "                    col_lens[i] -= 1\n",
    "            candidates = [i for i, le in enumerate(col_lens) if le <= 8]\n",
    "            if len(candidates) > 0:\n",
    "                idx = random.choice(candidates)\n",
    "                start, end = col_starts[idx], col_ends[idx]\n",
    "                new_tokens = tokens[:start] + tokens[end+1:]\n",
    "                new_labels = labels[:start] + labels[end+1:]\n",
    "            else:\n",
    "                new_tokens, new_labels = tokens, labels\n",
    "        else:\n",
    "            new_tokens, new_labels = tokens, labels\n",
    "\n",
    "        return new_tokens, new_labels\n",
    "    def augment_sent(self, text, op='all'):\n",
    "        \"\"\" Performs data augmentation on a classification example.\n",
    "\n",
    "        Similar to augment(tokens, labels) but works for sentences\n",
    "        or sentence-pairs.\n",
    "\n",
    "        Args:\n",
    "            text (str): the input sentence\n",
    "            op (str, optional): a string encoding of the operator to be applied\n",
    "\n",
    "        Returns:\n",
    "            str: the augmented sentence\n",
    "        \"\"\"\n",
    "        # 50% of chance of flipping\n",
    "        if ' [SEP] ' in text and random.randint(0, 1) == 0:\n",
    "            left, right = text.split(' [SEP] ')\n",
    "            text = right + ' [SEP] ' + left\n",
    "\n",
    "        # tokenize the sentence\n",
    "        current = ''\n",
    "        tokens = text.split(' ')\n",
    "\n",
    "        # avoid the special tokens\n",
    "        labels = []\n",
    "        for token in tokens:\n",
    "            if token in ['COL', 'VAL']:\n",
    "                labels.append('HD')\n",
    "            elif token in ['[CLS]', '[SEP]']:\n",
    "                labels.append('[SEP]')\n",
    "            else:\n",
    "                labels.append('O')\n",
    "\n",
    "        if op == 'all':\n",
    "            # RandAugment: https://arxiv.org/pdf/1909.13719.pdf\n",
    "            N = 3\n",
    "            ops = ['del', 'swap', 'drop_col', 'append_col']\n",
    "            for op in random.choices(ops, k=N):\n",
    "                tokens, labels = self.augment(tokens, labels, op=op)\n",
    "        else:\n",
    "            tokens, labels = self.augment(tokens, labels, op=op)\n",
    "        results = ' '.join(tokens)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T08:09:04.527402Z",
     "iopub.status.busy": "2025-08-04T08:09:04.527112Z",
     "iopub.status.idle": "2025-08-04T08:09:04.546158Z",
     "shell.execute_reply": "2025-08-04T08:09:04.545651Z",
     "shell.execute_reply.started": "2025-08-04T08:09:04.527385Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_tokenizer(lm):\n",
    "    if lm in lm_mp:\n",
    "        return AutoTokenizer.from_pretrained(lm_mp[lm])\n",
    "    else:\n",
    "        return AutoTokenizer.from_pretrained(lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T08:09:04.547111Z",
     "iopub.status.busy": "2025-08-04T08:09:04.546882Z",
     "iopub.status.idle": "2025-08-04T08:09:04.564130Z",
     "shell.execute_reply": "2025-08-04T08:09:04.563582Z",
     "shell.execute_reply.started": "2025-08-04T08:09:04.547091Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DittoDataset(data.Dataset):\n",
    "    \"\"\"EM Dataset\"\"\"\n",
    "    def __init__(self,\n",
    "                 path,\n",
    "                 max_len=256,\n",
    "                 size=None,\n",
    "                 lm='roberta',\n",
    "                 da=None):\n",
    "        self.tokenizer = get_tokenizer(lm)\n",
    "        self.pairs = []\n",
    "        self.labels = []\n",
    "        self.max_len = max_len\n",
    "        self.size = size\n",
    "\n",
    "        if isinstance(path, list):\n",
    "            lines = path\n",
    "        else:\n",
    "            with open(path, encoding=\"utf-8\") as f:\n",
    "                lines = f.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            s1, s2, label = line.strip().split('\\t')\n",
    "            self.pairs.append((s1, s2))\n",
    "            self.labels.append(int(label))\n",
    "\n",
    "        self.pairs = self.pairs[:size] # if size = None the list remains unchanged in length\n",
    "        self.labels = self.labels[:size]\n",
    "\n",
    "        self.da = da\n",
    "        if self.da is not None:\n",
    "            self.augmenter = Augmenter()\n",
    "        else:\n",
    "            self.augmenter = None\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the size of the dataset\"\"\"\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return a tokenized item of the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx (int): the index of the item\n",
    "\n",
    "        Returns:\n",
    "            List of int: token ID's of the two entities\n",
    "            List of int: token ID's of the two entities augmented (if da is set)\n",
    "            int: the label of the pair (0: unmatch, 1: match)\n",
    "        \"\"\"\n",
    "        left = self.pairs[idx][0]\n",
    "        right = self.pairs[idx][1]\n",
    "        \n",
    "        # left + right\n",
    "        x = self.tokenizer.encode(text=left,\n",
    "                                  text_pair=right,\n",
    "                                  max_length=self.max_len,\n",
    "                                  truncation=True)\n",
    "\n",
    "        if self.da is not None:\n",
    "            try:\n",
    "                combined = self.augmenter.augment_sent(left + ' [SEP] ' + right, self.da)\n",
    "                left, right = combined.split(' [SEP] ')\n",
    "                x_aug = self.tokenizer.encode(text=left,\n",
    "                                      text_pair=right,\n",
    "                                      max_length=self.max_len,\n",
    "                                      truncation=True)\n",
    "                return x, x_aug, self.labels[idx]\n",
    "            except:\n",
    "                x_aug = x\n",
    "                return x, x_aug, self.labels[idx]\n",
    "            \n",
    "        else:\n",
    "            return x, self.labels[idx]\n",
    "\n",
    "    @staticmethod\n",
    "    def pad(batch):\n",
    "        \"\"\"Merge a list of dataset items into a train/test batch\n",
    "        Args:\n",
    "            batch (list of tuple): a list of dataset items\n",
    "\n",
    "        Returns:\n",
    "            LongTensor: x1 of shape (batch_size, seq_len)\n",
    "            LongTensor: x2 of shape (batch_size, seq_len).\n",
    "                        Elements of x1 and x2 are padded to the same length\n",
    "            LongTensor: a batch of labels, (batch_size,)\n",
    "        \"\"\"\n",
    "\n",
    "        if len(batch[0]) == 3:\n",
    "            x1, x2, y = zip(*batch)\n",
    "\n",
    "            maxlen = max([len(x) for x in x1 + x2])\n",
    "            x1 = [xi + [0]*(maxlen - len(xi)) for xi in x1]\n",
    "            x2 = [xi + [0]*(maxlen - len(xi)) for xi in x2]\n",
    "\n",
    "            return torch.LongTensor(x1), torch.LongTensor(x2), torch.LongTensor(y)\n",
    "\n",
    "        else:\n",
    "            x12, y = zip(*batch)\n",
    "            maxlen = max([len(xi) for xi in x12])\n",
    "            x12 = [xi + [0]*(maxlen - len(xi)) for xi in x12]\n",
    "\n",
    "            return torch.LongTensor(x12), torch.LongTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T08:09:04.565055Z",
     "iopub.status.busy": "2025-08-04T08:09:04.564899Z",
     "iopub.status.idle": "2025-08-04T08:09:04.582840Z",
     "shell.execute_reply": "2025-08-04T08:09:04.582259Z",
     "shell.execute_reply.started": "2025-08-04T08:09:04.565042Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DittoModel(nn.Module):\n",
    "    \"\"\"A baseline model for EM.\"\"\"\n",
    "\n",
    "    def __init__(self, device=\"cuda\", lm=\"roberta\", alpha_aug=0.8):\n",
    "        super().__init__()\n",
    "        if lm in lm_mp:\n",
    "            self.bert = AutoModel.from_pretrained(lm_mp[lm])\n",
    "        else:\n",
    "            self.bert = AutoModel.from_pretrained(lm)\n",
    "\n",
    "        self.device = device\n",
    "        self.alpha_aug = alpha_aug\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        self.fc = torch.nn.Linear(hidden_size, 2)\n",
    "\n",
    "    def forward(self, x1, x2=None):\n",
    "        \"\"\"Encode the left, right, and the concatenation of left+right.\n",
    "\n",
    "        Args:\n",
    "            x1 (LongTensor): a batch of ID's\n",
    "            x2 (LongTensor, optional): a batch of ID's (augmented)\n",
    "\n",
    "        Returns:\n",
    "            Tensor: binary prediction\n",
    "        \"\"\"\n",
    "        if x2 is not None:\n",
    "            # MixDA\n",
    "            enc = self.bert(torch.cat((x1, x2)))[0][:, 0, :]\n",
    "            batch_size = len(x1)\n",
    "            enc1 = enc[:batch_size] # (batch_size, emb_size)\n",
    "            enc2 = enc[batch_size:] # (batch_size, emb_size)\n",
    "\n",
    "            aug_lam = np.random.beta(self.alpha_aug, self.alpha_aug)\n",
    "            enc = enc1 * aug_lam + enc2 * (1.0 - aug_lam)\n",
    "        else:\n",
    "            enc = self.bert(x1)[0][:, 0, :]\n",
    "\n",
    "        return self.fc(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T08:09:04.583830Z",
     "iopub.status.busy": "2025-08-04T08:09:04.583602Z",
     "iopub.status.idle": "2025-08-04T08:09:04.601260Z",
     "shell.execute_reply": "2025-08-04T08:09:04.600591Z",
     "shell.execute_reply.started": "2025-08-04T08:09:04.583807Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, threshold=None):\n",
    "    \"\"\"Evaluate a model on a validation/test dataset\n",
    "\n",
    "    Args:\n",
    "        model (DMModel): the EM model\n",
    "        iterator (Iterator): the valid/test dataset iterator\n",
    "        threshold (float, optional): the threshold on the 0-class\n",
    "\n",
    "    Returns:\n",
    "        float: the F1 score\n",
    "        float (optional): if threshold is not provided, the threshold\n",
    "            value that gives the optimal F1\n",
    "    \"\"\"\n",
    "    all_y = []\n",
    "    all_probs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            x, y = batch\n",
    "            x, y = x.to(model.device), y.to(model.device)\n",
    "            logits = model(x)\n",
    "            probs = logits.softmax(dim=1)[:, 1]\n",
    "            all_probs += probs.cpu().numpy().tolist()\n",
    "            all_y += y.cpu().numpy().tolist()\n",
    "\n",
    "    if threshold is not None:\n",
    "        pred = [1 if p > threshold else 0 for p in all_probs]\n",
    "        precision = metrics.precision_score(all_y, pred, zero_division=0)\n",
    "        recall = metrics.recall_score(all_y, pred, zero_division=0)\n",
    "        acc = metrics.accuracy_score(all_y, pred)\n",
    "        f1 = metrics.f1_score(all_y, pred, zero_division=0)\n",
    "        return f1, precision, recall, acc\n",
    "\n",
    "    best_th = 0.5\n",
    "    best_f1 = 0.0\n",
    "    best_precision = 0.0\n",
    "    best_recall = 0.0\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for th in np.arange(0.0, 1.0, 0.05):\n",
    "        pred = [1 if p > th else 0 for p in all_probs]\n",
    "        new_f1 = metrics.f1_score(all_y, pred, zero_division=0)\n",
    "        if new_f1 > best_f1:\n",
    "            best_f1 = new_f1\n",
    "            best_th = th\n",
    "            best_precision = metrics.precision_score(all_y, pred, zero_division=0)\n",
    "            best_recall = metrics.recall_score(all_y, pred, zero_division=0)\n",
    "            best_acc = metrics.accuracy_score(all_y, pred)\n",
    "\n",
    "    return best_f1, best_th, best_acc, best_precision, best_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T08:09:04.604565Z",
     "iopub.status.busy": "2025-08-04T08:09:04.604340Z",
     "iopub.status.idle": "2025-08-04T08:09:04.625984Z",
     "shell.execute_reply": "2025-08-04T08:09:04.625474Z",
     "shell.execute_reply.started": "2025-08-04T08:09:04.604550Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "def train_step(train_iter, model, optimizer, scheduler, hp):\n",
    "    \"\"\"Perform a single training step\n",
    "\n",
    "    Args:\n",
    "        train_iter (Iterator): the train data loader\n",
    "        model (DMModel): the model\n",
    "        optimizer (Optimizer): the optimizer (Adam or AdamW)\n",
    "        scheduler (LRScheduler): learning rate scheduler\n",
    "        hp (Namespace): other hyper-parameters (e.g., fp16)\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    scaler = GradScaler() if hp.fp16 else None\n",
    "    \n",
    "    for i, batch in enumerate(tqdm(train_iter, desc=\"Training\", leave=False)):\n",
    "        optimizer.zero_grad()\n",
    "        if len(batch) == 2:\n",
    "            x, y = batch\n",
    "            x, y = x.to(model.device, non_blocking=True), y.to(model.device, non_blocking=True)\n",
    "        else:\n",
    "            x1, x2, y = batch\n",
    "            x1 = x1.to(model.device, non_blocking=True)\n",
    "            x2 = x2.to(model.device, non_blocking=True)\n",
    "            y = y.to(model.device, non_blocking=True)\n",
    "\n",
    "        with autocast(\"cuda\", enabled=hp.fp16):\n",
    "            if len(batch) == 2:\n",
    "                prediction = model(x)\n",
    "            else:\n",
    "                prediction = model(x1, x2)\n",
    "            loss = criterion(prediction, y)\n",
    "            \n",
    "        if hp.fp16:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "        count += 1\n",
    "        \n",
    "        del loss\n",
    "    return total_loss / count if count > 0 else 0.0\n",
    "\n",
    "def train(trainset, validset, testset, run_tag, hp):\n",
    "    \"\"\"Train and evaluate the model\n",
    "\n",
    "    Args:\n",
    "        trainset (DittoDataset): the training set\n",
    "        validset (DittoDataset): the validation set\n",
    "        testset (DittoDataset): the test set\n",
    "        run_tag (str): the tag of the run\n",
    "        hp (Namespace): Hyper-parameters (e.g., batch_size,\n",
    "                        learning rate, fp16)\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    padder = trainset.pad\n",
    "\n",
    "    train_iter = data.DataLoader(dataset=trainset,\n",
    "                                 batch_size=hp.batch_size,\n",
    "                                 shuffle=True,\n",
    "                                 num_workers=0,\n",
    "                                 pin_memory=True,\n",
    "                                 collate_fn=padder)\n",
    "    valid_iter = data.DataLoader(dataset=validset,\n",
    "                                 batch_size=hp.batch_size*16,\n",
    "                                 shuffle=False,\n",
    "                                 num_workers=0,\n",
    "                                 pin_memory=True,\n",
    "                                 collate_fn=padder)\n",
    "    test_iter = data.DataLoader(dataset=testset,\n",
    "                                 batch_size=hp.batch_size*16,\n",
    "                                 shuffle=False,\n",
    "                                 num_workers=0,\n",
    "                                 pin_memory=True,\n",
    "                                 collate_fn=padder)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = DittoModel(device, lm = hp.lm, alpha_aug=hp.alpha_aug)\n",
    "    model = model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=hp.lr)\n",
    "\n",
    "    num_steps = (len(trainset) // hp.batch_size) * hp.epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0,\n",
    "                                                num_training_steps=num_steps)\n",
    "\n",
    "    best_dev_f1 = best_test_f1 = 0.0\n",
    "    \n",
    "    lm_name = hp.lm.replace('/', '_').replace('-', '_')\n",
    "    csv_filename = f\"bs{hp.batch_size}_ep{hp.epochs}_lm{lm_name}_alpha{hp.alpha_aug}.csv\"\n",
    "    csv_log_path = os.path.join(hp.base_path_blocking, hp.logdir, hp.task, csv_filename)\n",
    "\n",
    "    os.makedirs(os.path.dirname(csv_log_path), exist_ok=True)\n",
    "\n",
    "    with open(csv_log_path, mode='w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            \"epoch\", \"epoch_time_sec\", \"learning_rate\", \"train_loss\",\n",
    "            \"val_accuracy\", \"val_precision\", \"val_recall\", \"val_f1\", \"threshold\",\n",
    "            \"test_accuracy\", \"test_precision\", \"test_recall\", \"test_f1\"\n",
    "        ])\n",
    "    \n",
    "    for epoch in range(1, hp.epochs+1):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = train_step(train_iter, model, optimizer, scheduler, hp)\n",
    "\n",
    "        model.eval()\n",
    "    \n",
    "        dev_f1, threshold, val_acc, val_precision, val_recall = evaluate(model, valid_iter)\n",
    "        test_f1, test_precision, test_recall, test_acc = evaluate(model, test_iter, threshold=threshold)\n",
    "\n",
    "        epoch_time = time.time() - start_time \n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        \n",
    "        if dev_f1 > best_dev_f1:\n",
    "            best_dev_f1 = dev_f1\n",
    "            best_test_f1 = test_f1\n",
    "            if hp.save_model:\n",
    "                # create the directory if not exist\n",
    "                directory = os.path.join(hp.base_path_blocking, hp.logdir, hp.task)\n",
    "                if not os.path.exists(directory):\n",
    "                    os.makedirs(directory)\n",
    "\n",
    "                # save the checkpoints for each component\n",
    "                ckpt_path = os.path.join(hp.base_path_blocking, hp.logdir, hp.task, 'model.pt')\n",
    "                ckpt = {'model': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                        'scheduler': scheduler.state_dict(),\n",
    "                        'threshold': threshold,\n",
    "                        'epoch': epoch}\n",
    "                torch.save(ckpt, ckpt_path)\n",
    "\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        print(f\"  Time: {epoch_time:.2f} seconds\")\n",
    "        print(f\"  Learning Rate: {current_lr:.8f}\")\n",
    "        print(f\"  Validation F1: {dev_f1:.4f} | Threshold: {threshold:.2f}\")\n",
    "        print(f\"  Validation Accuracy: {val_acc:.4f}\")\n",
    "        print(f\"  Validation Precision: {val_precision:.4f}\")\n",
    "        print(f\"  Validation Recall: {val_recall:.4f}\")\n",
    "        print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "        print(f\"  Test Precision: {test_precision:.4f}\")\n",
    "        print(f\"  Test Recall: {test_recall:.4f}\")\n",
    "        print(f\"  Test F1 Score: {test_f1:.4f} | Best F1 so far: {best_test_f1:.4f}\")\n",
    "        with open(csv_log_path, mode='a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\n",
    "                epoch, round(epoch_time, 2), current_lr, round(train_loss, 4),\n",
    "                round(val_acc, 4), round(val_precision, 4), round(val_recall, 4), round(dev_f1, 4), round(threshold, 4),\n",
    "                round(test_acc, 4), round(test_precision, 4), round(test_recall, 4), round(test_f1, 4)\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T08:09:04.626775Z",
     "iopub.status.busy": "2025-08-04T08:09:04.626619Z",
     "iopub.status.idle": "2025-08-04T08:09:04.648174Z",
     "shell.execute_reply": "2025-08-04T08:09:04.647473Z",
     "shell.execute_reply.started": "2025-08-04T08:09:04.626763Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Summarizer:\n",
    "    \"\"\"To summarize a data entry pair into length up to the max sequence length.\n",
    "\n",
    "    Args:\n",
    "        task_config (Dictionary): the task configuration\n",
    "        lm (string): the language model (bert, albert, or distilbert)\n",
    "\n",
    "    Attributes:\n",
    "        config (Dictionary): the task configuration\n",
    "        tokenizer (Tokenizer): a tokenizer from the huggingface library\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, task_config, lm):\n",
    "        self.config = task_config\n",
    "        self.tokenizer = get_tokenizer(lm)\n",
    "        self.len_cache = {}\n",
    "        self.build_index()\n",
    "        \n",
    "    def build_index(self):\n",
    "        \"\"\"Build the idf index.\n",
    "\n",
    "        Store the index and vocabulary in self.idf and self.vocab.\n",
    "        \"\"\"\n",
    "        fns = [self.config[\"trainset\"],\n",
    "               self.config[\"validset\"],\n",
    "               self.config['testset']]\n",
    "        content = []\n",
    "        for fn in fns:\n",
    "            with open(fn) as fin:\n",
    "                for line in fin:\n",
    "                    LL = line.split('\\t')\n",
    "                    if len(LL) > 2:\n",
    "                        for entry in LL:\n",
    "                            content.append(entry)\n",
    "        vectorizer = TfidfVectorizer().fit(content)\n",
    "        self.vocab = vectorizer.vocabulary_\n",
    "        self.idf = vectorizer.idf_\n",
    "\n",
    "    def get_len(self, word):\n",
    "        \"\"\"\n",
    "        Return the sentence_piece length of a token.\n",
    "        \"\"\"\n",
    "        if word in self.len_cache:\n",
    "            return self.len_cache[word]\n",
    "        length = len(self.tokenizer.tokenize(word))\n",
    "        self.len_cache[word] = length\n",
    "        return length\n",
    "    \n",
    "    def transform(self, row, max_len=128):\n",
    "        \"\"\"\n",
    "        Summarize one single example.\n",
    "\n",
    "        Only retain tokens of the highest tf-idf.\n",
    "\n",
    "        Args:\n",
    "            row (str): a matching example of two data entries and a binary label, separated by tab\n",
    "            max_len (int, Optional): the maximum sequence length to be summerized to\n",
    "        \n",
    "        Returns:\n",
    "            str: the summarized example\n",
    "        \"\"\"\n",
    "        sentA, sentB, label = row.strip().split('\\t')\n",
    "        res=''\n",
    "        cnt = Counter()\n",
    "        for sent in [sentA, sentB]:\n",
    "            tokens = sent.split(' ')\n",
    "            for token in tokens:\n",
    "                if token not in ['COL', 'VAL'] and token not in stopwords:\n",
    "                    if token in self.vocab:\n",
    "                        cnt[token] += self.idf[self.vocab[token]]\n",
    "        \n",
    "\n",
    "        for sent in [sentA, sentB]:\n",
    "            token_cnt = Counter(sent.split(' '))\n",
    "            total_len = token_cnt['VAL'] + token_cnt['COL']\n",
    "\n",
    "            subset = Counter()\n",
    "            for token in set(token_cnt.keys()):\n",
    "                subset[token] = cnt[token]\n",
    "            subset = subset.most_common(max_len)\n",
    "\n",
    "            topk_tokens_copy = set([])\n",
    "            for word, _ in subset:\n",
    "                bert_len = self.get_len(word)\n",
    "                if bert_len + total_len > max_len:\n",
    "                    break\n",
    "                total_len += bert_len\n",
    "                topk_tokens_copy.add(word)\n",
    "            \n",
    "            for token in sent.split(' '):\n",
    "                if token in ['COL', 'VAL']:\n",
    "                    res += token + ' '\n",
    "                elif token in topk_tokens_copy:\n",
    "                    res += token + ' '\n",
    "                    topk_tokens_copy.remove(token)\n",
    "\n",
    "            res += '\\t'\n",
    "\n",
    "        res += label + '\\n'\n",
    "\n",
    "        return res\n",
    "    \n",
    "    def transform_file(self, input_fn, max_len = 256, overwrite = False):\n",
    "        \"\"\"Summarize all lines of a tsv file.\n",
    "\n",
    "        Run the summarizer. If the output already exists, just return the file name.\n",
    "\n",
    "        Args:\n",
    "            input_fn (str): the input file name\n",
    "            max_len (int, optional): the max sequence len\n",
    "            overwrite (bool, optional): if true, then overwrite any cached output\n",
    "\n",
    "        Returns:\n",
    "            str: the output file name\n",
    "        \"\"\"\n",
    "        out_fn = input_fn.replace(\"input\", \"working\") + '.su'\n",
    "        os.makedirs(os.path.dirname(out_fn), exist_ok=True)\n",
    "        if not os.path.exists(out_fn) or os.stat(out_fn).st_size == 0 or overwrite:\n",
    "            with open(out_fn, \"w\") as fout:\n",
    "                for line in open(input_fn):\n",
    "                    fout.write(self.transform(line, max_len = max_len))\n",
    "        \n",
    "        return out_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T08:09:04.649595Z",
     "iopub.status.busy": "2025-08-04T08:09:04.649310Z",
     "iopub.status.idle": "2025-08-04T08:09:04.669891Z",
     "shell.execute_reply": "2025-08-04T08:09:04.669163Z",
     "shell.execute_reply.started": "2025-08-04T08:09:04.649578Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DKInjector:\n",
    "    \"\"\"Inject domain knowledge to the data entry pairs.\n",
    "\n",
    "    Attributes:\n",
    "        config: the task configuration\n",
    "        name: the injector name\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name):\n",
    "        self.config = config\n",
    "        self.name = name\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, entry):\n",
    "        return entry\n",
    "\n",
    "    def transform_file(self, input_fn, overwrite=False):\n",
    "        \"\"\"Transform all lines of a tsv file.\n",
    "\n",
    "        Run the knowledge injector. If the output already exists, just return the file name.\n",
    "\n",
    "        Args:\n",
    "            input_fn (str): the input file name\n",
    "            overwrite (bool, optional): if true, then overwrite any cached output\n",
    "\n",
    "        Returns:\n",
    "            str: the output file name\n",
    "        \"\"\"\n",
    "        out_fn = input_fn + '.dk'\n",
    "        if not os.path.exists(out_fn) or \\\n",
    "            os.stat(out_fn).st_size == 0 or overwrite:\n",
    "\n",
    "            with open(out_fn, 'w') as fout:\n",
    "                for line in open(input_fn):\n",
    "                    LL = line.split('\\t')\n",
    "                    if len(LL) == 3:\n",
    "                        entry0 = self.transform(LL[0])\n",
    "                        entry1 = self.transform(LL[1])\n",
    "                        fout.write(entry0 + '\\t' + entry1 + '\\t' + LL[2].strip() + '\\n')\n",
    "        return out_fn\n",
    "\n",
    "\n",
    "class ProductDKInjector(DKInjector):\n",
    "    \"\"\"The domain-knowledge injector for product data.\n",
    "    \"\"\"\n",
    "    def initialize(self):\n",
    "        \"\"\"Initialize spacy\"\"\"\n",
    "        self.nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "    def transform(self, entry):\n",
    "        \"\"\"Transform a data entry.\n",
    "\n",
    "        Use NER to regconize the product-related named entities and\n",
    "        mark them in the sequence. Normalize the numbers into the same format.\n",
    "\n",
    "        Args:\n",
    "            entry (str): the serialized data entry\n",
    "\n",
    "        Returns:\n",
    "            str: the transformed entry\n",
    "        \"\"\"\n",
    "        res = ''\n",
    "        doc = self.nlp(entry, disable=['parser'])\n",
    "        ents = doc.ents\n",
    "        start_indices = {}\n",
    "        end_indices = {}\n",
    "\n",
    "        for ent in ents:\n",
    "            start, end, label = ent.start, ent.end, ent.label_\n",
    "            if label in ['NORP', 'GPE', 'LOC', 'PERSON', 'PRODUCT']:\n",
    "                start_indices[start] = 'PRODUCT'\n",
    "                end_indices[end] = 'PRODUCT'\n",
    "            if label in ['DATE', 'QUANTITY', 'TIME', 'PERCENT', 'MONEY']:\n",
    "                start_indices[start] = 'NUM'\n",
    "                end_indices[end] = 'NUM'\n",
    "\n",
    "        for idx, token in enumerate(doc):\n",
    "            if idx in start_indices:\n",
    "                res += start_indices[idx] + ' '\n",
    "\n",
    "            # normalizing the numbers\n",
    "            if token.like_num:\n",
    "                try:\n",
    "                    val = float(token.text)\n",
    "                    if val == round(val):\n",
    "                        res += '%d ' % (int(val))\n",
    "                    else:\n",
    "                        res += '%.2f ' % (val)\n",
    "                except:\n",
    "                    res += token.text + ' '\n",
    "            elif len(token.text) >= 7 and \\\n",
    "                 any([ch.isdigit() for ch in token.text]):\n",
    "                res += 'ID ' + token.text + ' '\n",
    "            else:\n",
    "                res += token.text + ' '\n",
    "        return res.strip()\n",
    "\n",
    "class GeneralDKInjector(DKInjector):\n",
    "    \"\"\"The domain-knowledge injector for publication and business data.\n",
    "    \"\"\"\n",
    "    def initialize(self):\n",
    "        \"\"\"Initialize spacy\"\"\"\n",
    "        self.nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "    def transform(self, entry):\n",
    "        \"\"\"Transform a data entry.\n",
    "\n",
    "        Use NER to regconize the product-related named entities and\n",
    "        mark them in the sequence. Normalize the numbers into the same format.\n",
    "\n",
    "        Args:\n",
    "            entry (str): the serialized data entry\n",
    "\n",
    "        Returns:\n",
    "            str: the transformed entry\n",
    "        \"\"\"\n",
    "        res = ''\n",
    "        doc = self.nlp(entry, disable=['parser'])\n",
    "        ents = doc.ents\n",
    "        start_indices = {}\n",
    "        end_indices = {}\n",
    "\n",
    "        for ent in ents:\n",
    "            start, end, label = ent.start, ent.end, ent.label_\n",
    "            if label in ['PERSON', 'ORG', 'LOC', 'PRODUCT', 'DATE', 'QUANTITY', 'TIME']:\n",
    "                start_indices[start] = label\n",
    "                end_indices[end] = label\n",
    "\n",
    "        for idx, token in enumerate(doc):\n",
    "            if idx in start_indices:\n",
    "                res += start_indices[idx] + ' '\n",
    "\n",
    "            # normalizing the numbers\n",
    "            if token.like_num:\n",
    "                try:\n",
    "                    val = float(token.text)\n",
    "                    if val == round(val):\n",
    "                        res += '%d ' % (int(val))\n",
    "                    else:\n",
    "                        res += '%.2f ' % (val)\n",
    "                except:\n",
    "                    res += token.text + ' '\n",
    "            elif len(token.text) >= 7 and \\\n",
    "                 any([ch.isdigit() for ch in token.text]):\n",
    "                res += 'ID ' + token.text + ' '\n",
    "            else:\n",
    "                res += token.text + ' '\n",
    "        return res.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T08:09:04.670884Z",
     "iopub.status.busy": "2025-08-04T08:09:04.670691Z",
     "iopub.status.idle": "2025-08-04T08:09:04.687335Z",
     "shell.execute_reply": "2025-08-04T08:09:04.686698Z",
     "shell.execute_reply.started": "2025-08-04T08:09:04.670863Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------------------------\n",
    "# TO UPDATE TO THE PROJECT ROOT\n",
    "base_path_blocking = \"D:/Study/ENSIAS/stage_2/ER/ditto/resultat\"\n",
    "# ---------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "hp = Namespace(\n",
    "    # Hyperparameters for blocking part\n",
    "    model_name_blocking=\"all-MiniLM-L12-v2\",\n",
    "    top_k_blocking=5,\n",
    "    threshold_blocking=0.95,\n",
    "    batch_size_blocking=512,\n",
    "\n",
    "    \n",
    "    # Paths\n",
    "    base_path_blocking=base_path_blocking,\n",
    "\n",
    "    # Input CSVs\n",
    "    table_reference_csv=f\"{base_path_blocking}/data/reference_table.csv\",\n",
    "    table_source_csv=f\"{base_path_blocking}/data/source_table.csv\",\n",
    "    ground_truth_csv=f\"{base_path_blocking}/data/ground_truth.csv\",\n",
    "\n",
    "    # Ditto-style TXT\n",
    "    table_reference_txt=f\"{base_path_blocking}/input_txt_blocking/reference_table.txt\",\n",
    "    table_source_txt=f\"{base_path_blocking}/input_txt_blocking/source_table.txt\",\n",
    "\n",
    "    # Vector files\n",
    "    table_reference_vec=f\"{base_path_blocking}/vectors_blocking/reference_table.txt.mat\",\n",
    "    table_source_vec=f\"{base_path_blocking}/vectors_blocking/source_table.txt.mat\",\n",
    "\n",
    "    # Blocking outputs\n",
    "    output_pairs_csv=f\"{base_path_blocking}/blocking/blocking_pairs.csv\",\n",
    "    output_ditto_txt=f\"{base_path_blocking}/blocking/blocking_pairs_ditto.txt\",\n",
    "\n",
    "    # Inference output\n",
    "    output_inference_csv=f\"{base_path_blocking}/inference/result.csv\",\n",
    "\n",
    "    dataset_csv_dir=f\"{base_path_blocking}/dataset_ditto_csv\",\n",
    "    dataset_txt_dir=f\"{base_path_blocking}/dataset_ditto_txt\",\n",
    "\n",
    "    # Logging and task info\n",
    "    logdir=\"./logs\",\n",
    "    task=\"Generated_data\",\n",
    "\n",
    "    # Hyperparameters for training\n",
    "    batch_size=32,\n",
    "    lr=3e-5,\n",
    "    epochs=2,\n",
    "    save_model=True,\n",
    "    lm=\"distilbert\",\n",
    "    size=None,\n",
    "    alpha_aug=0.8,\n",
    "    max_len=256,\n",
    "    da=\"all\",\n",
    "    summarize=True,\n",
    "    dk=True,\n",
    "    fp16=True,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "\n",
    "configs = [{\n",
    "    \"name\": \"Generated_data\",\n",
    "    \"trainset\": f\"{hp.base_path_blocking}/dataset_ditto_txt/train.txt\",\n",
    "    \"validset\": f\"{hp.base_path_blocking}/dataset_ditto_txt/valid.txt\",\n",
    "    \"testset\": f\"{hp.base_path_blocking}/dataset_ditto_txt/test.txt\"\n",
    "}]\n",
    "\n",
    "\n",
    "configs = {conf['name'] : conf for conf in configs}\n",
    "config = configs[hp.task]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T08:09:04.689022Z",
     "iopub.status.busy": "2025-08-04T08:09:04.688219Z",
     "iopub.status.idle": "2025-08-04T08:09:04.704047Z",
     "shell.execute_reply": "2025-08-04T08:09:04.703431Z",
     "shell.execute_reply.started": "2025-08-04T08:09:04.688996Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_full_pipeline(hp, config):\n",
    "    trainset = config['trainset']\n",
    "    validset = config['validset']\n",
    "    testset = config['testset']\n",
    "    random.seed(42)  # For reproducibility\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    #TO UPDATE IF NEEDED FOR DATA GENERATION\n",
    "    generate_tables(base_path=hp.base_path_blocking, n_total=500, match_ratio=0.3)\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    run_blocking(hp)\n",
    "    if hp.summarize:\n",
    "        summarizer = Summarizer(config, hp.lm)\n",
    "        trainset = summarizer.transform_file(trainset, max_len = hp.max_len, overwrite=hp.overwrite)\n",
    "        testset = summarizer.transform_file(testset, max_len = hp.max_len, overwrite=hp.overwrite)\n",
    "        validset = summarizer.transform_file(validset, max_len = hp.max_len, overwrite=hp.overwrite)\n",
    "\n",
    "    if hp.dk is not None:\n",
    "        if hp.dk == 'product':\n",
    "            injector = ProductDKInjector(config, hp.dk)\n",
    "        else:\n",
    "            injector = GeneralDKInjector(config, hp.dk)\n",
    "        \n",
    "        trainset = injector.transform_file(trainset, overwrite=hp.overwrite)\n",
    "        validset = injector.transform_file(validset, overwrite=hp.overwrite)\n",
    "        testset = injector.transform_file(testset, overwrite=hp.overwrite)\n",
    "\n",
    "    train_dataset = DittoDataset(trainset,\n",
    "                                   lm=hp.lm,\n",
    "                                   max_len=hp.max_len,\n",
    "                                   size=hp.size,\n",
    "                                   da=hp.da)\n",
    "    valid_dataset = DittoDataset(validset, lm=hp.lm)\n",
    "    test_dataset = DittoDataset(testset, lm=hp.lm)\n",
    "\n",
    "    t1 = time.time()\n",
    "    train(train_dataset, valid_dataset, test_dataset, run_tag=\"test_run\", hp=hp)\n",
    "    t2 = time.time()\n",
    "\n",
    "    print(f\"Trainig time: {round(t2-t1, 3)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T08:09:04.705296Z",
     "iopub.status.busy": "2025-08-04T08:09:04.704787Z",
     "iopub.status.idle": "2025-08-04T08:09:04.721526Z",
     "shell.execute_reply": "2025-08-04T08:09:04.720963Z",
     "shell.execute_reply.started": "2025-08-04T08:09:04.705272Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_model_and_threshold(model_path, device, lm=hp.lm):\n",
    "    model = DittoModel(device=device, lm=lm)\n",
    "    ckpt = torch.load(model_path, map_location=device, weights_only=False)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    threshold = ckpt.get(\"threshold\", 0.5)\n",
    "    return model, threshold\n",
    "\n",
    "def predict(model, tokenizer, left_str, right_str, device, threshold, max_len=hp.max_len):\n",
    "    encoded = tokenizer.encode(text=left_str,\n",
    "                               text_pair=right_str,\n",
    "                               max_length=max_len,\n",
    "                               truncation=True)\n",
    "    encoded = torch.LongTensor(encoded).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(encoded)\n",
    "        probs = logits.softmax(dim=1)\n",
    "        match_prob = probs[0][1].item()\n",
    "        prediction = int(match_prob > threshold)\n",
    "\n",
    "    return prediction, match_prob\n",
    "\n",
    "\n",
    "def run_inference(model_path, left_str, right_str, lm=hp.lm, alpha_aug=hp.alpha_aug, threshold=None):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, saved_threshold = load_model_and_threshold(model_path, device, lm)\n",
    "\n",
    "    if threshold is None:\n",
    "        threshold = saved_threshold\n",
    "\n",
    "    tokenizer = get_tokenizer(lm)\n",
    "\n",
    "    pred, prob = predict(model, tokenizer, left_str, right_str, device, threshold, max_len=hp.max_len)\n",
    "\n",
    "    print(\"prediction: \", pred)\n",
    "    print(\"probability: \", prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T08:09:04.722339Z",
     "iopub.status.busy": "2025-08-04T08:09:04.722119Z",
     "iopub.status.idle": "2025-08-04T08:15:43.810266Z",
     "shell.execute_reply": "2025-08-04T08:15:43.809490Z",
     "shell.execute_reply.started": "2025-08-04T08:09:04.722324Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "run_full_pipeline(hp, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T08:15:43.812026Z",
     "iopub.status.busy": "2025-08-04T08:15:43.811782Z",
     "iopub.status.idle": "2025-08-04T08:15:46.599610Z",
     "shell.execute_reply": "2025-08-04T08:15:46.598963Z",
     "shell.execute_reply.started": "2025-08-04T08:15:43.812002Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if hp.save_model == False:\n",
    "    print(\"Model should be saved for inference\")\n",
    "else:\n",
    "    \n",
    "    model_path = f\"{hp.base_path_blocking}/logs/Generated_data/model.pt\"\n",
    "    left_str = \"COL full_name VAL Michelle Andre COL cin VAL EX717542 COL date_of_birth VAL 1991-02-12 COL place_of_birth VAL Tanger COL cnss_number VAL 36759250 COL email VAL isaacriviere@example.org COL phone VAL +33 3 28 68 25 81 COL address VAL 9, rue Blot COL city VAL Marrakech COL employer_name VAL Menard\"\n",
    "    right_str = \"COL full_name VAL Clémence Parent Le Rey COL cin VAL IR469929 COL date_of_birth VAL 1994-06-27 COL place_of_birth VAL Kenitra COL cnss_number VAL 25591412 COL email VAL jweber@example.org COL phone VAL 01 45 59 74 83 COL address VAL rue de Marchand COL city VAL Marrakech COL employer_name VAL Mendès Potier et Fils\"\n",
    "    run_inference(model_path, left_str, right_str)\n",
    "    print(\"--------------------------------------\")\n",
    "    left_str=\"COL full_name VAL Audrey Brunet COL cin VAL XE204809 COL date_of_birth VAL 1964-01-04 COL place_of_birth VAL Agadir COL cnss_number VAL 29199351 COL email VAL marcelle91@example.org COL phone VAL +33 7 93 72 16 30 COL address VAL 97, boulevard Colin COL city VAL Marrakech COL employer_name VAL Guillet\"\n",
    "    right_str=\"COL full_name VAL audrey brunet COL cin VAL XE204809 COL date_of_birth VAL 1964-01-04 COL place_of_birth VAL Agadir COL cnss_number VAL nan COL email VAL marcelle91@example.org COL phone VAL g33 7 93 72 16 30 COL address VAL 97, boulevard Colin COL city VAL Marrakech COL employer_name VAL Guillet\"\n",
    "    \n",
    "    run_inference(model_path, left_str, right_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T08:15:46.600788Z",
     "iopub.status.busy": "2025-08-04T08:15:46.600538Z",
     "iopub.status.idle": "2025-08-04T08:15:46.609915Z",
     "shell.execute_reply": "2025-08-04T08:15:46.609148Z",
     "shell.execute_reply.started": "2025-08-04T08:15:46.600762Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_blocked_inference(model_path=os.path.join(hp.base_path_blocking, hp.logdir, hp.task, 'model.pt'),\n",
    "                         blocked_pairs_csv=hp.output_pairs_csv,\n",
    "                         reference_table_csv=hp.table_reference_csv,\n",
    "                         source_table_csv=hp.table_source_csv,\n",
    "                         output_csv=hp.output_inference_csv,\n",
    "                         lm=hp.lm,\n",
    "                         max_len=hp.max_len):\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, threshold = load_model_and_threshold(model_path, device, lm)\n",
    "    tokenizer = get_tokenizer(lm)\n",
    "\n",
    "    ref_df = pd.read_csv(reference_table_csv)\n",
    "    src_df = pd.read_csv(source_table_csv)\n",
    "    blocked_pairs = pd.read_csv(blocked_pairs_csv)\n",
    "\n",
    "    from collections import defaultdict\n",
    "    grouped = defaultdict(list)\n",
    "\n",
    "\n",
    "    for _, row in blocked_pairs.iterrows():\n",
    "        grouped[row[\"id_table_b\"]].append((row[\"id_table_a\"], row[\"id_table_b\"]))\n",
    "\n",
    "\n",
    "    results = []\n",
    "    for idx2, pairs in tqdm(grouped.items(), desc=\"Processing blocked inference\"):\n",
    "        best_prob = -1.0\n",
    "        best_result = None\n",
    "\n",
    "        for idx1, idx2 in pairs:\n",
    "            left_str = ref_df.loc[idx1].astype(str).str.cat(sep=' ')\n",
    "            right_str = src_df.loc[idx2].astype(str).str.cat(sep=' ')\n",
    "            pred, prob = predict(model, tokenizer, left_str, right_str, device, threshold, max_len)\n",
    "\n",
    "            if pred == 1 and prob > best_prob:\n",
    "                best_prob = prob\n",
    "                best_result = {\n",
    "                    \"idx1\": idx1,\n",
    "                    \"idx2\": idx2,\n",
    "                    \"probability\": prob,\n",
    "                    \"predicted_label\": pred,\n",
    "                    \"ref_row\": ref_df.loc[idx1].to_dict(),\n",
    "                    \"src_row\": src_df.loc[idx2].to_dict()\n",
    "                }\n",
    "\n",
    "        if best_result:\n",
    "            # Flatten the row data into output format\n",
    "            row = {\n",
    "                \"idx1\": best_result[\"idx1\"],\n",
    "                \"idx2\": best_result[\"idx2\"],\n",
    "                \"probability\": best_result[\"probability\"],\n",
    "                \"predicted_label\": best_result[\"predicted_label\"]\n",
    "            }\n",
    "            # Add table columns with prefixes\n",
    "            for col, val in best_result[\"ref_row\"].items():\n",
    "                row[f\"ref_{col}\"] = val\n",
    "            for col, val in best_result[\"src_row\"].items():\n",
    "                row[f\"src_{col}\"] = val\n",
    "\n",
    "            results.append(row)\n",
    "\n",
    "    # Write to output CSV\n",
    "    output_df = pd.DataFrame(results)\n",
    "    os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "    output_df.to_csv(output_csv, index=False)\n",
    "    print(f\"\\nSaved final results to: {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T08:15:46.610859Z",
     "iopub.status.busy": "2025-08-04T08:15:46.610675Z",
     "iopub.status.idle": "2025-08-04T08:16:33.106662Z",
     "shell.execute_reply": "2025-08-04T08:16:33.105961Z",
     "shell.execute_reply.started": "2025-08-04T08:15:46.610844Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "run_blocked_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T08:16:33.108003Z",
     "iopub.status.busy": "2025-08-04T08:16:33.107780Z",
     "iopub.status.idle": "2025-08-04T08:16:33.119759Z",
     "shell.execute_reply": "2025-08-04T08:16:33.119150Z",
     "shell.execute_reply.started": "2025-08-04T08:16:33.107987Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_metrics(csv_path, save_dir=None):\n",
    "    \"\"\"\n",
    "    Plots training and evaluation metrics over epochs.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path to the CSV file containing logged metrics.\n",
    "        save_dir (str, optional): Directory to save the plot image.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    epochs = df[\"epoch\"]\n",
    "\n",
    "    num_rows = 3\n",
    "    num_cols = 3\n",
    "    plt.figure(figsize=(18, 12))\n",
    "\n",
    "    # 1. F1 Scores\n",
    "    plt.subplot(num_rows, num_cols, 1)\n",
    "    plt.plot(epochs, df[\"val_f1\"], label=\"Validation F1\", marker='o')\n",
    "    plt.plot(epochs, df[\"test_f1\"], label=\"Test F1\", marker='o')\n",
    "    plt.title(\"F1 Scores\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 2. Accuracy\n",
    "    plt.subplot(num_rows, num_cols, 2)\n",
    "    plt.plot(epochs, df[\"val_accuracy\"], label=\"Validation Accuracy\", marker='o')\n",
    "    plt.plot(epochs, df[\"test_accuracy\"], label=\"Test Accuracy\", marker='o')\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 3. Precision\n",
    "    plt.subplot(num_rows, num_cols, 3)\n",
    "    plt.plot(epochs, df[\"val_precision\"], label=\"Validation Precision\", marker='o')\n",
    "    plt.plot(epochs, df[\"test_precision\"], label=\"Test Precision\", marker='o')\n",
    "    plt.title(\"Precision\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 4. Recall\n",
    "    plt.subplot(num_rows, num_cols, 4)\n",
    "    plt.plot(epochs, df[\"val_recall\"], label=\"Validation Recall\", marker='o')\n",
    "    plt.plot(epochs, df[\"test_recall\"], label=\"Test Recall\", marker='o')\n",
    "    plt.title(\"Recall\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Recall\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 5. Threshold\n",
    "    plt.subplot(num_rows, num_cols, 5)\n",
    "    plt.plot(epochs, df[\"threshold\"], label=\"Threshold\", marker='o', color='purple')\n",
    "    plt.title(\"Threshold over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Threshold\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 6. Train Loss\n",
    "    if \"train_loss\" in df.columns:\n",
    "        plt.subplot(num_rows, num_cols, 6)\n",
    "        plt.plot(epochs, df[\"train_loss\"], label=\"Train Loss\", marker='o', color='brown')\n",
    "        plt.title(\"Training Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "    # 7. Learning Rate\n",
    "    if \"learning_rate\" in df.columns:\n",
    "        plt.subplot(num_rows, num_cols, 7)\n",
    "        plt.plot(epochs, df[\"learning_rate\"], label=\"Learning Rate\", marker='o', color='green')\n",
    "        plt.title(\"Learning Rate\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"LR\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "    # 8. Epoch Time\n",
    "    if \"epoch_time_sec\" in df.columns:\n",
    "        plt.subplot(num_rows, num_cols, 8)\n",
    "        plt.plot(epochs, df[\"epoch_time_sec\"], label=\"Epoch Time (sec)\", marker='o', color='orange')\n",
    "        plt.title(\"Epoch Duration\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Seconds\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_dir:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        filename = os.path.basename(csv_path).replace(\".csv\", \".png\")\n",
    "        plot_path = os.path.join(save_dir, filename)\n",
    "        plt.savefig(plot_path)\n",
    "        print(f\"[✔] Plot saved to {plot_path}\")\n",
    "    else:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T08:16:33.120735Z",
     "iopub.status.busy": "2025-08-04T08:16:33.120497Z",
     "iopub.status.idle": "2025-08-04T08:16:34.495747Z",
     "shell.execute_reply": "2025-08-04T08:16:34.494930Z",
     "shell.execute_reply.started": "2025-08-04T08:16:33.120719Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lm_name = hp.lm.replace('/', '_').replace('-', '_')\n",
    "csv_filename = f\"bs{hp.batch_size}_ep{hp.epochs}_lm{lm_name}_alpha{hp.alpha_aug}.csv\"\n",
    "csv_log_path = os.path.join(hp.base_path_blocking ,hp.logdir, hp.task, csv_filename)\n",
    "plot_metrics(csv_log_path)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ditto_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
